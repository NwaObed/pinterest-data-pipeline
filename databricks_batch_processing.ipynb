{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59ac976-c57e-4b48-b20d-0602c54831ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8857b594-a2ff-4c73-b284-cf67524feded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#AWS cred locations\n",
    "dbutils.fs.ls(\"/FileStore/tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6554ab6d-2340-40c5-a9e1-f156cc3fb82e",
     "showTitle": true,
     "title": "Read Credentials"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_type = \"csv\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    "                    .option(\"header\", first_row_is_header)\\\n",
    "                    .option(\"sep\", delimiter)\\\n",
    "                    .load(\"/FileStore/tables/authentication_credentials.csv\")\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d4d575-5a31-4126-b536-b0eb7c731b05",
     "showTitle": true,
     "title": "Mount S3"
    }
   },
   "outputs": [],
   "source": [
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"user-12853887c065-bucket\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/mount_name\"\n",
    "# Source url\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive\n",
    "# dbutils are not supported outside of databricks notebook\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME) # RUN ONCE ONLY!\n",
    "\n",
    "display(dbutils.fs.ls(\"/mnt/mount_name/topics/12853887c065.geo/partition=0/\"))\n",
    "\n",
    "dbutils.fs.unmount(\"/mnt/mount_name\") #unmount if it is already mounted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop config and streaming from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8b8bfc-f86d-4fd7-91a3-11596a74a85c",
     "showTitle": true,
     "title": "Hadoop config and streaming from S3"
    }
   },
   "outputs": [],
   "source": [
    "# Adding the packages required to get data from S3  \n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.amazonaws:aws-java-sdk-s3:1.12.490,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell\"\n",
    "\n",
    "# Configure the hadoop setting to read from the S3 bucket\n",
    "hadoopConf.set('fs.s3a.access.key', ACCESS_KEY)\n",
    "hadoopConf.set('fs.s3a.secret.key', SECRET_KEY)\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') # Allows the package to authenticate with AWS\n",
    "\n",
    "# Stream from the S3 bucket\n",
    "s3_bucket_url = \"s3n://user-12853887c065-bucket/topics/12853887c065\"\n",
    "extension = \"/partition=0/12853887c065\"\n",
    "while True:\n",
    "    df_pin = spark.read.json(f\"{s3_bucket_url}.pin{extension}.pin+0+*.json\")\n",
    "    df_geo = spark.read.json(f\"{s3_bucket_url}.geo{extension}.geo+0+*.json\")\n",
    "    df_user = spark.read.json(f\"{s3_bucket_url}.user{extension}.user+0+*.json\")\n",
    "    df_pin.show(5, True)\n",
    "    df_geo.show(5, True)\n",
    "    df_user.show(5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Pin post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42fe38ac-75f7-4b8c-93ba-ed1a972f36b5",
     "showTitle": true,
     "title": "Cleaning Pin post"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Check for null values\n",
    "df_pin.filter(f.greatest(*[f.col(i).isNull() for i in df_pin.columns])).show()\n",
    "\n",
    "# Perform the necessary transformations on the follower_count to ensure every entry is a number. Make sure the data type of this column is an int.\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(df_pin[\"follower_count\"], 'k', ''))\n",
    "\n",
    "#Cast Data types on columns\n",
    "df_pin = df_pin.withColumn(\"follower_count\",col(\"follower_count\").cast(\"int\"))\n",
    "df_pin = df_pin.withColumn(\"downloaded\",col(\"downloaded\").cast(\"int\"))\n",
    "df_pin = df_pin.withColumn(\"index\",col(\"index\").cast(\"int\"))\n",
    "\n",
    "#Update the follower_count to 1000\n",
    "df_pin = df_pin.withColumn('follower_count', df_pin.follower_count*1000)\n",
    "\n",
    "# Clean the data in the save_location column to include only the save location path\n",
    "df_pin = df_pin.withColumn('save_location', split(df_pin.save_location, ' ').getItem(3))\n",
    "\n",
    "# Rename the index column to ind.\n",
    "df_pin = df_pin.withColumnRenamed(\"index\",\"ind\")\n",
    "\n",
    "# Reorder the DataFrame columns to have the following column order:\n",
    "new_column = ['ind', 'unique_id', 'title', 'description', 'follower_count', 'poster_name', 'tag_list', 'is_image_or_video', 'image_src', 'save_location', 'category']\n",
    "df_pin = df_pin.select(new_column)\n",
    "\n",
    "df_pin.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e52194b5-2042-442d-9c76-3948453a7d31",
     "showTitle": true,
     "title": "Cleaning geolocation"
    }
   },
   "outputs": [],
   "source": [
    "#Create a new column coordinates that contains an array based on the latitude and longitude columns\n",
    "df_geo = df_geo.withColumn('coordinates', array('latitude', 'longitude'))\n",
    "\n",
    "#Drop the latitude and longitude columns from the DataFrame\n",
    "df_geo = df_geo.drop('latitude', 'longitude')\n",
    "\n",
    "#Convert the timestamp column from a string to a timestamp data type\n",
    "df_geo = df_geo.withColumn('timestamp', col('timestamp').cast('timestamp'))\n",
    "\n",
    "#Reorder the DataFrame columns to have the following column order:\n",
    "col_order = ['ind', 'country', 'coordinates', 'timestamp']\n",
    "df_geo = df_geo.select(col_order)\n",
    "\n",
    "df_geo.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Users Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818844dc-b6e8-4c15-8d45-bbf4767c3eda",
     "showTitle": true,
     "title": "Cleaning Users Info"
    }
   },
   "outputs": [],
   "source": [
    "#Create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "df_user = df_user.withColumn('user_name', concat_ws(' ', 'first_name', 'last_name'))\n",
    "\n",
    "#Drop the first_name and last_name columns from the DataFrame\n",
    "# df_user = df_user.drop('first_name', 'last_name')\n",
    "\n",
    "#Convert the date_joined column from a string to a timestamp data type\n",
    "df_user = df_user.withColumn('date_joined', col('date_joined').cast('timestamp'))\n",
    "\n",
    "# Reorder the DataFrame columns to have the following column order:\n",
    "user_col = ['ind', 'user_name', 'age', 'date_joined']\n",
    "df_user = df_user.select(user_col)\n",
    "df_user.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb9bebd-2c0d-4b4c-b6cf-6c4addc000ab",
     "showTitle": true,
     "title": "Task 4"
    }
   },
   "outputs": [],
   "source": [
    "# Find the most popular Pinterest category people post to based on their country.\n",
    "# First join the dataframe\n",
    "df_pin_geo = df_pin.join(df_geo, df_pin.ind==df_geo.ind, 'inner')\n",
    "\n",
    "#Groupby country and category\n",
    "df_pin_geo.groupby('country','category') \\\n",
    "    .agg(count('category')\\\n",
    "        .alias('category_count'))\\\n",
    "    .sort('category_count', ascending=False)\\\n",
    "    .show()\n",
    "\n",
    "# df_pin_geo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dfd5bdb-8f55-4857-abde-434474765781",
     "showTitle": true,
     "title": "Task 5 with SQL"
    }
   },
   "outputs": [],
   "source": [
    "# Find how many posts each category had between 2018 and 2022.\n",
    "\n",
    "#Create temporary view \n",
    "df_pin_geo.createOrReplaceTempView(\"pin_geo\")\n",
    "\n",
    "# Use spark sql queries\n",
    "spark.sql('SELECT YEAR(timestamp) as post_year,\\\n",
    "                    category,\\\n",
    "                    count(category) category_count\\\n",
    "           FROM pin_geo\\\n",
    "            WHERE YEAR(timestamp) BETWEEN 2018 AND 2022\\\n",
    "            GROUP BY post_year, category\\\n",
    "            ORDER BY post_year, category_count DESC')\\\n",
    "            .show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 with window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec40328-8564-47cc-b03d-a85d5b2d62fb",
     "showTitle": true,
     "title": "Task 5 with window function"
    }
   },
   "outputs": [],
   "source": [
    "# Find how many posts each category had between 2018 and 2022.\n",
    "\n",
    "windowSpec = Window.partitionBy(df_pin_geo.category)\\\n",
    "                    .orderBy(df_pin_geo.timestamp)\n",
    "\n",
    "df_pin_geo.filter((year('timestamp') >= 2018) & (year('timestamp') <= 2022))\\\n",
    "            .select(year('timestamp').alias('post_year'), 'category', count('category').over(windowSpec).alias('category_count')).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a59c535-2aec-4e03-8133-425ebe9324f3",
     "showTitle": true,
     "title": "Task 6 with SQL"
    }
   },
   "outputs": [],
   "source": [
    "#Find the user with most followers\n",
    "\n",
    "# Step 1: For each country find the user with the most followers.\n",
    "# df_country_poster_follower = df_pin_geo.select('country', 'poster_name', 'follower_count')\n",
    "\n",
    "spark.sql(\"WITH most_follower_table AS (\\\n",
    "                SELECT pin_geo.country, \\\n",
    "                    pin_geo.poster_name, \\\n",
    "                    SUM(pin_geo.follower_count) total_follower\\\n",
    "                FROM pin_geo\\\n",
    "                GROUP BY pin_geo.country, pin_geo.poster_name\\\n",
    "        )\\\n",
    "    SELECT country,\\\n",
    "        sum(total_follower) follower_count\\\n",
    "    FROM most_follower_table\\\n",
    "    GROUP BY country\\\n",
    "    ORDER BY follower_count DESC\\\n",
    "    LIMIT 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 with Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a10cfe-61b0-479f-ab32-038595286ec8",
     "showTitle": true,
     "title": "Task 6 with Groupby"
    }
   },
   "outputs": [],
   "source": [
    "df_pin_geo.select(df_pin_geo.country, df_pin_geo.poster_name, df_pin_geo.follower_count)\\\n",
    "            .groupby(df_pin_geo.country)\\\n",
    "            .agg(sum(df_pin_geo.follower_count).alias('follower_count'))\\\n",
    "            .orderBy(col('follower_count').desc()).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "533c0e5c-50c5-428a-b148-7bc0b06e3fd3",
     "showTitle": true,
     "title": "Task 7 with SQL"
    }
   },
   "outputs": [],
   "source": [
    "# What is the most popular category people post to based on the following age groups: 18-24, 25-35, 36-50, +50\n",
    "#Join df_pin and df_user\n",
    "df_pin_user = df_pin.join(df_user, df_pin.ind==df_user.ind, 'inner')\n",
    "\n",
    "#Create temporary view \n",
    "df_pin_user.createOrReplaceTempView(\"pin_user\")\n",
    "spark.sql(\"SELECT\\\n",
    "    CASE\\\n",
    "        WHEN age BETWEEN 18 AND 24 THEN '18-24' \\\n",
    "        WHEN age BETWEEN 25 AND 35 THEN '25-35'\\\n",
    "        WHEN age BETWEEN 36 AND 50 THEN '36-50'\\\n",
    "        WHEN age > 50 THEN '50+'\\\n",
    "    END AS age_group,\\\n",
    "    category,\\\n",
    "    COUNT(category) AS category_count\\\n",
    "    FROM pin_user\\\n",
    "    GROUP BY age_group, category\\\n",
    "    ORDER BY age_group ASC, category_count DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee19deea-9b03-41af-b4e2-dbff6ca09895",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\\\n",
    "        WITH category_table AS (\\\n",
    "            SELECT\\\n",
    "                CASE\\\n",
    "                    WHEN age BETWEEN 18 AND 24 THEN '18-24' \\\n",
    "                    WHEN age BETWEEN 25 AND 35 THEN '25-35'\\\n",
    "                    WHEN age BETWEEN 36 AND 50 THEN '36-50'\\\n",
    "                    WHEN age > 50 THEN '50+'\\\n",
    "                END AS age_group,\\\n",
    "                category,\\\n",
    "                COUNT(category) category_count\\\n",
    "            FROM pin_user\\\n",
    "            GROUP BY age_group, category\\\n",
    "            )\\\n",
    "        SELECT\\\n",
    "            age_group,\\\n",
    "            category,\\\n",
    "            MAX(category_count)\\\n",
    "        FROM category_table\\\n",
    "        GROUP BY age_group, category\"\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 with Window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "593e1f99-9910-467f-b1f1-991a90b9b302",
     "showTitle": true,
     "title": "Task 7 with Window function"
    }
   },
   "outputs": [],
   "source": [
    "# What is the most popular category people post to based on the following age groups: 18-24, 25-35, 36-50, +50\n",
    "\n",
    "#Add age-group col\n",
    "df_pin_user = df_pin_user.withColumn('age_group', when((df_pin_user.age >= 18) & (df_pin_user.age <=25), '18-25')\n",
    "                                                    .when((df_pin_user.age >= 26) & (df_pin_user.age <= 35), '26-35')\n",
    "                                                    .when((df_pin_user.age >= 36) & (df_pin_user.age <= 50), '36-50')\n",
    "                                                    .otherwise('50+'))\n",
    "\n",
    "#Window specification\n",
    "pin_user_windowSpec = Window.partitionBy('age_group')\\\n",
    "                            .orderBy('category')\n",
    "\n",
    "\n",
    "\n",
    "df_pin_user.select('age_group', 'category', count(col('category')).over(pin_user_windowSpec).alias('category_count')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8 with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d86f86-4f95-4cad-9024-3d2355f57d61",
     "showTitle": true,
     "title": "Task 8 with SQL"
    }
   },
   "outputs": [],
   "source": [
    "# What is the median follower count for users in the following age groups: 18-24, 25-35, 36-50, +50\n",
    "\n",
    "spark.sql(\"\\\n",
    "    SELECT\\\n",
    "            CASE\\\n",
    "                WHEN age BETWEEN 18 AND 24 THEN '18-24' \\\n",
    "                WHEN age BETWEEN 25 AND 35 THEN '25-35'\\\n",
    "                WHEN age BETWEEN 36 AND 50 THEN '36-50'\\\n",
    "                WHEN age > 50 THEN '50+'\\\n",
    "            END AS age_group,\\\n",
    "            round(mean(follower_count), 3) mean_follower_count,\\\n",
    "            max(follower_count) min_follower_count,\\\n",
    "            min(follower_count) min_follower_count\\\n",
    "        FROM pin_user\\\n",
    "        GROUP BY age_group\\\n",
    "        ORDER BY age_group\\\n",
    "    \").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8 with Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83d35907-8d36-47bf-8df4-96f9a378cab7",
     "showTitle": true,
     "title": "Task 8 with \"when other\" syntax"
    }
   },
   "outputs": [],
   "source": [
    "df_pin_user.groupby('age_group')\\\n",
    "            .agg(sum('follower_count').alias('total_follower_count'), round(mean('follower_count'), 3).alias('mean_follower_count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ecb0e4-59e7-49b2-b7dc-a456acc93aed",
     "showTitle": true,
     "title": "Task 9"
    }
   },
   "outputs": [],
   "source": [
    "#Find how many users have joined between 2015 and 2020.\n",
    "\n",
    "# Join user and geo df\n",
    "df_geo_user = df_geo.join(df_user, df_geo.ind==df_user.ind, 'inner')\n",
    "\n",
    "# df_geo_user.withColumn('timestamp', YEAR(col('timestamp'))).show()\n",
    "\n",
    "df_geo_user.groupby(year('timestamp').alias('post_year'))\\\n",
    "            .agg(count(year('timestamp')).alias('number_users_joined'))\\\n",
    "            .sort('number_users_joined')\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "701d1e16-1842-4675-ba29-010ea0365d68",
     "showTitle": true,
     "title": "Task 10"
    }
   },
   "outputs": [],
   "source": [
    "# Find the median follower count of users have joined between 2015 and 2020.\n",
    "df_pin_geo.filter((year('timestamp') >= 2015) & (year('timestamp') <= 2020))\\\n",
    "            .groupby(year('timestamp').alias('post_year')).agg(round(mean('follower_count') ,2).alias('median_follower_count'))\\\n",
    "            .orderBy('post_year').show() #median function not defined. Testing with mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7391359-f6a5-4ad7-bea0-08438ab5f79b",
     "showTitle": true,
     "title": "Task 11"
    }
   },
   "outputs": [],
   "source": [
    "#Find the median follower count of users that have joined between 2015 and 2020, based on which age group they are part of.\n",
    "# Join pin, geo & user df\n",
    "df_pin_geo_user = df_pin.join(df_geo, df_pin.ind==df_geo.ind, 'inner')\\\n",
    "                        .join(df_user, df_geo.ind==df_user.ind, 'inner')\n",
    "\n",
    "#Add age-group col\n",
    "df_pin_geo_user = df_pin_geo_user.withColumn('age_group', when((df_pin_geo_user.age >= 18) & (df_pin_geo_user.age <=25), '18-25')\n",
    "                                                            .when((df_pin_geo_user.age >= 26) & (df_pin_geo_user.age <= 35), '26-35')\n",
    "                                                            .when((df_pin_geo_user.age >= 36) & (df_pin_geo_user.age <= 50), '36-50')\n",
    "                                                            .otherwise('50+'))\n",
    "df_pin_geo_user.filter((year('timestamp') >= 2015) & (year('timestamp') <= 2020))\\\n",
    "            .groupby('age_group',year('timestamp').alias('post_year')).agg(mean('follower_count').alias('median_follower_count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 11 with Window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "690c6f4b-d5e5-4143-bdde-0839a19b7b43",
     "showTitle": true,
     "title": "Task 11 with Window function"
    }
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy('age_group')\\\n",
    "                    .orderBy('timestamp')\n",
    "\n",
    "# median function not imported, testing with mean\n",
    "df_pin_geo_user.filter((year('timestamp') >= 2015) & (year('timestamp') <= 2020))\\\n",
    "                .select('age_group', year('timestamp').alias('post_year'), mean('follower_count').over(windowSpec).alias('mean_follower_count')).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2023-06-18 14:01:25",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
