{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f577c080-862b-4bd3-818a-6c219174b58d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "510d60ef-3ffe-4941-8862-ca275562951e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c114c380-1ee8-4173-b2af-97c8f03d03a7",
     "showTitle": true,
     "title": "Read Streaming data"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stream pin post\n",
    "df_pin = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-12853887c065-pin') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "# display(df_pin)\n",
    "\n",
    "# Stream geolocation\n",
    "df_geo = spark\\\n",
    "    .readStream\\\n",
    "    .format('kinesis')\\\n",
    "    .option('streamName', 'streaming-12853887c065-geo')\\\n",
    "    .option('initialPosition', 'earliest')\\\n",
    "    .option('region', 'us-east-1')\\\n",
    "    .option('awsAccessKey', ACCESS_KEY)\\\n",
    "    .option('awsSecretKey', SECRET_KEY)\\\n",
    "    .load()\n",
    "\n",
    "# df_geo.display()\n",
    "\n",
    "#Stream user\n",
    "df_user = spark\\\n",
    "    .readStream\\\n",
    "    .format('kinesis')\\\n",
    "    .option('streamName', 'streaming-12853887c065-user')\\\n",
    "    .option('initialPosition', 'earliest')\\\n",
    "    .option('region', 'us-east-1')\\\n",
    "    .option('awsAccessKey', ACCESS_KEY)\\\n",
    "    .option('awsSecretKey', SECRET_KEY)\\\n",
    "    .load()\n",
    "# df_user.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert streaming data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5761bd36-9f2e-4e7d-a51c-d575fe7a8226",
     "showTitle": true,
     "title": "Convert streaming data to dataframe"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cast to string to read json\n",
    "df_pin = df_pin.selectExpr(\"CAST(data as STRING)\")\n",
    "df_geo = df_geo.selectExpr(\"CAST(data as STRING)\")\n",
    "df_user = df_user.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "###Construct schema\n",
    "# pin schema\n",
    "df_pin_schema = StructType([\\\n",
    "    StructField(\"index\", IntegerType(),True),\\\n",
    "    StructField(\"unique_id\", StringType(),True),\\\n",
    "    StructField(\"title\", StringType(),True),\\\n",
    "    StructField(\"follower_count\", StringType(),True),\\\n",
    "    StructField(\"poster_name\", StringType(),True),\\\n",
    "    StructField(\"tag_list\", StringType(),True),\\\n",
    "    StructField(\"is_image_or_video\", StringType(),True),\\\n",
    "    StructField(\"image_src\", StringType(),True),\\\n",
    "    StructField(\"save_location\", StringType(),True),\\\n",
    "    StructField(\"category\", StringType(),True),\\\n",
    "    StructField(\"downloaded\", IntegerType(),True),\\\n",
    "    StructField(\"description\", StringType(),True)\\\n",
    "])\n",
    "\n",
    "\n",
    "#geo schema\n",
    "df_geo_schema = StructType([\\\n",
    "    StructField(\"ind\", IntegerType(),True),\\\n",
    "    StructField(\"country\", StringType(),True),\\\n",
    "    StructField(\"latitude\", StringType(),True),\\\n",
    "    StructField(\"longitude\", StringType(),True),\\\n",
    "    StructField(\"timestamp\", StringType(),True),\\\n",
    "])\n",
    "\n",
    "#geo schema\n",
    "df_user_schema = StructType([\\\n",
    "    StructField(\"ind\", IntegerType(),True),\\\n",
    "    StructField(\"first_name\", StringType(),True),\\\n",
    "    StructField(\"last_name\", StringType(),True),\\\n",
    "    StructField(\"age\", StringType(),True),\\\n",
    "    StructField(\"date_joined\", StringType(),True),\\\n",
    "])\n",
    "\n",
    "\n",
    "#Convert json to dataframe using the defined schemas\n",
    "\n",
    "df_pin = df_pin.withColumn(\"data\", from_json(col(\"data\"),df_pin_schema))\\\n",
    "                .select(\"data.*\") # data : Kinesis streaming data header name\n",
    "\n",
    "#geo\n",
    "df_geo = df_geo.withColumn('data', from_json(col('data'), df_geo_schema)).select('data.*')\n",
    "\n",
    "#user\n",
    "df_user = df_user.withColumn('data', from_json(col('data'), df_user_schema)).select('data.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Pin post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c24c78-2143-4264-9103-be90bb31c809",
     "showTitle": true,
     "title": "Cleaning Pin post"
    }
   },
   "outputs": [],
   "source": [
    "# Cast follower_count column to int\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(df_pin[\"follower_count\"], 'k', ''))\n",
    "\n",
    "df_pin = df_pin.withColumn(\"follower_count\", col('follower_count').cast('int'))\n",
    "df_pin = df_pin.withColumn(\"downloaded\",col(\"downloaded\").cast(\"int\"))\n",
    "df_pin = df_pin.withColumn(\"index\",col(\"index\").cast(\"int\"))\n",
    "\n",
    "#Update the follower_count to 1000\n",
    "df_pin = df_pin.withColumn('follower_count', df_pin.follower_count*1000) #run once\n",
    "\n",
    "#Replace null values\n",
    "df_pin = df_pin.fillna(0)\n",
    "\n",
    "#Replace null values with None\n",
    "\n",
    "# Clean the data in the save_location column to include only the save location path\n",
    "df_pin = df_pin.withColumn('save_location', split(df_pin.save_location, ' ').getItem(3))\n",
    "\n",
    "#Rename the index column to ind.\n",
    "df_pin = df_pin.withColumn('ind', col('index'))\n",
    "\n",
    "df_pin_header = ['ind',\n",
    "  'unique_id',\n",
    "  'title',\n",
    "  'description',\n",
    "  'follower_count',\n",
    "  'poster_name',\n",
    "  'tag_list',\n",
    "  'is_image_or_video',\n",
    "  'image_src',\n",
    "  'save_location',\n",
    "  'category']\n",
    "\n",
    "df_pin = df_pin.select(df_pin_header)\n",
    "\n",
    "df_pin.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1479650-231e-4b24-a0c1-0b71658822a7",
     "showTitle": true,
     "title": "Cleaning geolocation"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new column coordinates that contains an array based on the latitude and longitude columns\n",
    "df_geo = df_geo.withColumn('coordinates', array('latitude', 'longitude'))\n",
    "\n",
    "# Drop the latitude and longitude columns from the DataFrame\n",
    "df_geo = df_geo.drop('latitude', 'longitude')\n",
    "\n",
    "# Convert the timestamp column from a string to a timestamp data type\n",
    "df_geo = df_geo.withColumn('timestamp', col('timestamp').cast('timestamp'))\n",
    "df_geo = df_geo.withColumn('timestamp', col('timestamp').cast('timestamp'))\n",
    "\n",
    "\n",
    "# Reorder the DataFrame columns to have the following column order:\n",
    "df_geo_header = [ 'ind',\n",
    "  'country',\n",
    "  'coordinates',\n",
    "  'timestamp']\n",
    "\n",
    "df_geo = df_geo.select(df_geo_header)\n",
    "df_geo.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6bd0af-2f3b-4abd-87e4-3991a23103dd",
     "showTitle": true,
     "title": "Cleaning user data"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "df_user = df_user.withColumn('user_name', concat('first_name', 'last_name'))\n",
    "\n",
    "# Drop the first_name and last_name columns from the DataFrame\n",
    "df_user = df_user.drop('first_name', 'last_name')\n",
    "\n",
    "# Convert the date_joined column from a string to a timestamp data type\n",
    "df_user = df_user.withColumn('date_joined', col('date_joined').cast('timestamp'))\n",
    "\n",
    "# Reorder the DataFrame columns to have the following column order:\n",
    "\n",
    "df_user_header = ['ind',\n",
    "  'user_name',\n",
    "  'age',\n",
    "  'date_joined']\n",
    "\n",
    "df_user = df_user.select(df_user_header)\n",
    "\n",
    "df_user.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the streaming data to Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9987204-1966-4641-b163-1c56053d1290",
     "showTitle": true,
     "title": "Write the streaming data to Delta Tables"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[151]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f0180939760&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[151]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f0180939760&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove the checkpoint folder first\n",
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)\n",
    "\n",
    "df_pin.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"12853887c065_pin_table\")\n",
    "\n",
    "\n",
    "df_geo.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"12853887c065_geo_table\")\n",
    "\n",
    "\n",
    "df_user.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"12853887c065_user_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "streaming-pin-post",
   "widgets": {}
  },
  "interpreter": {
   "hash": "5ab6cb1f5c7dfb9724c34bd82b58dded0b44f6750f81d47867e6b3ef07c3b88e"
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 ('pinterest_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
